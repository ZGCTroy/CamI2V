<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="CamI2V, a camera-controllable image-to-video diffusion model enhanced by explicit geometry constraint.">
    <meta property="og:title" content="CamI2V: Camera-Controlled Image-to-Video Diffusion Model" />
    <meta property="og:description" content="CamI2V, a camera-controllable image-to-video diffusion model enhanced by explicit geometry constraint." />
    <meta property="og:url" content="https://zgctroy.github.io/CamI2V/" />

    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" /> -->


    <meta name="twitter:title" content="CamI2V: Camera-Controlled Image-to-Video Diffusion Model">
    <meta name="twitter:description" content="CamI2V, a camera-controllable image-to-video diffusion model enhanced by explicit geometry constraint.">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
    <!-- <meta name="twitter:card" content="summary_large_image"> -->

    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="CamI2V, Camera Control, Image-to-Video, Video Diffusion Model">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>CamI2V: Camera-Controlled Image-to-Video Diffusion Model</title>
    <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:ital,wght@1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/dreampulse/computer-modern-web-font@master/fonts.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>

    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$','$'], ['\\(','\\)']]
            }
        });
    </script>

    <style>
        .video-container {
            display: flex;
            justify-content: center;
            gap: 0px;
        }

        .italic {
            font-family: 'Playfair Display';
            font-style: italic;
        }
    </style>
</head>

<body>
    <!-- title and author -->
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">CamI2V: Camera-Controlled Image-to-Video Diffusion Model</h1>
                        <!-- <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">First Author</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                            </span>
                        </div> -->

                        <!-- <div class="is-size-5 publication-authors">
                            <span class="author-block">Institution Name<br>Conferance name and year</span>
                            <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                        </div> -->

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">*Under Review, WIP</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a href="https://arxiv.org/pdf/2410.15957.pdf" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <a href="https://github.com/ZGCTroy/CamI2V" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2410.15957" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- first image -->
    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <img src="https://github.com/user-attachments/assets/65f2d8c4-9d5a-4443-8173-762e14329041" width="100%" />
                <h2 class="subtitle has-text-justified">
                    <b>Rethinking condition in diffusion models.</b>
                    Diffusion models denoise along the gradient of log probability density function.
                    At large noise levels, the high density region becomes the overlap of numerous noisy samples, resulting in visual blurriness.
                    We point out that <b><em style="font-family: Times New Roman;">the effectiveness of a condition depends on how much uncertainty it reduces</em></b>.
                    From a new perspective, we categorize conditions into <b><em style="font-family: Times New Roman;">clean conditions</em></b> (e.g. texts, camera extrinsics) that remain visible throughout the denoising process, and <b><em style="font-family: Times New Roman;">noised conditions</em></b> (e.g. noised pixels in the current and other frames) whose deterministic information $\alpha_t x_0$ will be gradually dominated by the randomness of noise $\sigma_t \epsilon$.
                </h2>
            </div>
        </div>
    </section>

    <!-- abstract -->
    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Recent advancements have integrated camera pose as a user-friendly and physics-informed condition in video diffusion models, enabling precise camera control.
                            In this paper, we identify one of the key challenges as effectively modeling noisy cross-frame interactions to enhance geometry consistency and camera controllability.
                            We innovatively associate the quality of a condition with its ability to reduce uncertainty and interpret noisy cross-frame features as a form of noisy condition.
                            Recognizing that noisy conditions provide deterministic information while also introducing randomness and potential misguidance due to added noise, we propose applying epipolar attention to only aggregate features along corresponding epipolar lines, thereby accessing an optimal amount of noisy conditions.
                            Additionally, we address scenarios where epipolar lines disappear, commonly caused by rapid camera movements, dynamic objects, or occlusions, ensuring robust performance in diverse environments.
                            Furthermore, we develop a more robust and reproducible evaluation pipeline to address the inaccuracies and instabilities of existing camera control metrics.
                            Our method achieves a 25.64% improvement in camera controllability on the RealEstate10K dataset without compromising dynamics or generation quality and demonstrates strong generalization to out-of-domain images.
                            Training and inference require only 24GB and 12GB of memory, respectively, for 16-frame sequences at 256\(\times\)256 resolution.
                            We will release all checkpoints, along with training and evaluation code.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- out of domain 1024x576-->
    <section class="section hero is-small">
        <div class="container has-text-centered">
            <h2 class="title is-3">Visualization on Out-of-domain Images (1024\(\times\)576)</h2>

            <h2 class="subtitle">
                Coming Soon ...
            </h2>

        </div>
    </section>

    <section class="section hero">

        <div class="container has-text-centered">
            <h2 class="title is-3">More Visualization on Out-of-domain Images (512\(\times\)320)</h2>
            <h3 class="subtitle">
                *Generated by 512\(\times\)320 model (50k training steps), compatible with input images of arbitary aspect ratio.
            </h3>

            <!-- Row 1 -->
            <div class="video-container">

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/d198291c-4bdb-4678-b396-5accbcf903c9" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Pan Left
                    </h2>
                </div>

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/258a13e3-1e14-4535-9e7f-017fa0639376" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Pan Right
                    </h2>
                </div>

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/314f44ed-14d9-4097-ae9f-4750f6a9157e" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Pan Up
                    </h2>
                </div>

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/51af2474-d1bb-4c90-85c9-e4312c38d093" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Pan Down
                    </h2>
                </div>

            </div>

            <br>

            <!-- Row 2 -->
            <div class="video-container">

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/2d96494e-2392-4879-88c5-36326923eaa7" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Look Left
                    </h2>
                </div>

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/632196ba-1b18-42f4-9116-26d36be52f22" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Look Right
                    </h2>
                </div>

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/fc1c601f-984b-4651-b614-5f8598b1839b" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Orbit Left
                    </h2>
                </div>

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/d71b78e7-cb20-45d2-9dea-2b17118ec226" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Orbit Right
                    </h2>
                </div>

            </div>

            <br>

            <!-- Row 3 -->
            <div class="video-container">

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/9b425c29-8d82-49d7-b11a-eb20883d33cb" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Zoom In & Rotate
                    </h2>
                </div>

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/3077d283-7bf3-449f-9a85-b6df8ffac569" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Pan Left & Zoom
                    </h2>
                </div>

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/a34e3cef-ab14-4ddb-a431-31979fff1ab5" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Forward &#8594; Backward
                    </h2>
                </div>

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/0e23cd12-2b4e-4d92-b796-d5329e67854e" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Walking
                    </h2>
                </div>


            </div>

        </div>

    </section>

    <!-- out of domain 512x320-->
    <section class="section hero">

        <div class="container has-text-centered">
            <h2 class="title is-3">Visualization on Out-of-domain Images (512\(\times\)320)</h2>
            <h3 class="subtitle">*Original outputs from 512\(\times\)320 model, no padding removed.</h3>

            <div class="video-container">

                <video autoplay controls muted loop>
                    <source src="https://github.com/user-attachments/assets/6c324a13-36bf-4ba5-af52-fae11e83f289" type="video/mp4" />
                </video>

            </div>

        </div>

    </section>

    <!-- out of domain 256x256-->
    <section class="section hero">

        <div class="container has-text-centered">
            <h2 class="title is-3">Visualization on Out-of-domain Images (256\(\times\)256)</h2>

            <div class="video-container">

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/d109d396-4d88-4f24-b165-c9fa50b02726" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Orbit Left
                    </h2>
                </div>

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/650033bd-04b5-4fb1-8c97-f45edd13271e" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Orbit Right
                    </h2>
                </div>

            </div>

            <br>

            <div class="video-container">

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/a0ab6e49-c1da-4768-b76d-8e2069cbe898" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Zoom In
                    </h2>
                </div>

                <div>
                    <video autoplay controls muted loop>
                        <source src="https://github.com/user-attachments/assets/9f9622c6-528a-4e3f-bf22-d9d8567cb7f5" type="video/mp4" />
                    </video>
                    <h2 class="subtitle has-text-centered italic">
                        Zoom Out
                    </h2>
                </div>

            </div>

        </div>

    </section>

    <!-- in domain -->
    <section class="section hero is-small">
        <div class="container has-text-centered">
            <h2 class="title is-3">Visualization on RealEstate10K (256\(\times\)256) </h2>

            <div class="video-container">

                <video autoplay controls muted loop>
                    <source src="https://github.com/user-attachments/assets/2b6c4b63-6957-4bf0-8591-c2fa42277bb6" type="video/mp4" />
                </video>

            </div>
        </div>
    </section>

    <!-- Quantitative Comparison -->
    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">Quantitative Comparison & Ablation Study</h2>
                <div class="content has-text-centered is-size-7">
                    <table>
                        <tr>
                            <td class="has-text-justified">Method</td>
                            <td>RotErr$\downarrow$</td>
                            <td>TransErr$\downarrow$</td>
                            <td>CamMC$\downarrow$</td>
                            <td>FVD$\downarrow$<br>(VideoGPT)</td>
                            <td>FVD$\downarrow$<br>(StyleGAN)</td>
                        </tr>
                        <tr>
                            <td class="has-text-justified">DynamiCrafter</td>
                            <td>3.3415</td>
                            <td>9.8024</td>
                            <td>11.625</td>
                            <td>106.02</td>
                            <td>92.196</td>
                        </tr>
                        <tr>
                            <td class="has-text-justified">+ MotionCtrl</td>
                            <td>0.8636</td>
                            <td>2.5068</td>
                            <td>2.9536</td>
                            <td>70.820</td>
                            <td>60.363</td>
                        </tr>
                        <tr>
                            <td class="has-text-justified">+ Plucker Embedding (Baseline, CameraCtrl)</td>
                            <td>0.7098</td>
                            <td>1.8877</td>
                            <td>2.2557</td>
                            <td><b>66.077</b></td>
                            <td>55.889</td>
                        </tr>
                        <tr>
                            <td class="has-text-justified">+ Plucker Embedding + Epipolar Attention Only on Reference Frame (CamCo-like)</td>
                            <td>0.5738</td>
                            <td>1.6014</td>
                            <td>1.8851</td>
                            <td>66.439</td>
                            <td>56.778</td>
                        </tr>
                        <tr>
                            <td class="has-text-justified">+ Plucker Embedding + Epipolar Attention (Our CamI2V)</td>
                            <td><b>0.4758</b></td>
                            <td><b>1.4955</b></td>
                            <td><b>1.7153</b></td>
                            <td>66.090</td>
                            <td><b>55.701</b></td>
                        </tr>
                        <tr>
                            <td class="has-text-justified">+ Plucker Embedding + 3D Full Attention</td>
                            <td>0.6299</td>
                            <td>1.8215</td>
                            <td>2.1315</td>
                            <td>71.026</td>
                            <td>60.00</td>
                        </tr>
                    </table>
                </div>
                <div class="content has-text-justified">
                    <p>
                        Adding more conditions to generative models typically reduces uncertainty and improves generation quality (e.g. providing detailed text conditions through recaption).
                        In this paper, we argue that it is also crucial to consider <b>noisy conditions</b> like latent features $z_t$, which contain valuable information along with random noise.
                        For instance, in SDEdit for image-to-image translation, random noise is added to the input $z_0$ to produce a noisy $z_t$.
                        The clean component $z_0$ preserves overall similarity, while the introduced noise leads to uncertainty, enabling diverse and varied generations.
                    </p>
                    <p>
                        In this paper, we argue that <b>providing the model with more noisy conditions, especially at high noise levels, does not necessarily reduce more uncertainty, as the noise also introduces randomness and misleadingness</b>.
                        This is the key insight we aim to convey.
                        To validate this point, we designed experiment with the following setups:
                    </p>
                    <ol>
                        <li>
                            <b>Pl端cker Embedding (Baseline):</b>
                            This setup, akin to CameraCtrl, has minimal noisy conditions on cross frames due to the inefficiency of the indirect cross-frame interaction (spatial and temporal attention).
                        </li>
                        <li>
                            <b>Pl端cker Embedding + Epipolar Attention only on reference frame:</b>
                            Similar to CamCo, this setup treats the reference frame as the source view, enabling the target frame to refer to it.
                            It accesses <b>a small amount</b> of noisy conditions on the reference frame.
                            However, some pixels of the current frame may have no epipolar line interacted with reference frame, causing it to degenerate to a CameraCtrl-like model without epipolar attention.
                        </li>
                        <li>
                            <b>Pl端cker Embedding + Epipolar Attention (Our CamI2V):</b>
                            This setup can impose epipolar constraints with all frames, including adjacent frames that have interactions in most cases to ensure an sufficient amount of noisy conditions.
                        </li>
                        <li>
                            <b>Pl端cker Embedding + 3D Full Attention:</b>
                            This configuration allows the model to directly interact with features of all other frames, accessing the most noisy conditions.
                        </li>
                    </ol>
                    <p>
                        <b>The amount of accessible noisy conditions of the above four setups increase progressively.</b>
                        One might expect that 3D full attention, which accesses the most noisy conditions, would achieve the best performance. However, as shown in the table, 3D full attention performs only slightly better than CameraCtrl and is inferior to CamCo-like setup who only applies epipolar attention on reference frame.
                        Notably, our method achieves best result by interacting with noisy conditions more along the epipolar lines.
                        It can be clearly seen below that CamCo-like setup reference too much on the first frame and cannot generate new objects.
                        3D full attention setup generates objects with large movement due to its access to pixels from all frames while the colors of specific pixels are affected by pixels of incorrect positions.
                        These findings confirm our insight that <b>an optimal amount of noisy conditions leads to better uncertainty reduction, rather than merely increasing the quantity of noisy conditions</b>.
                    </p>
                </div>
            </div>
        </div>

        <!-- ablation 1 -->
        <div class="video-container">

            <div>
                <video autoplay controls muted loop>
                    <source src="https://github.com/user-attachments/assets/074b5771-86cd-48f0-bb9e-91cb23b21717" type="video/mp4" />
                </video>
                <h2 class="subtitle has-text-centered italic">
                    CamI2V (Ours)
                </h2>
            </div>

            <div>
                <video autoplay controls muted loop>
                    <source src="https://github.com/user-attachments/assets/7e77e037-4e68-48d7-a8a8-12e50ffc14ce" type="video/mp4" />
                </video>
                <h2 class="subtitle has-text-centered italic">
                    CamI2V - 3D full attention
                </h2>
            </div>
    
            <div>
                <video autoplay controls muted loop>
                    <source src="https://github.com/user-attachments/assets/8934b1a9-6316-4654-bddb-674f0f08a75b" type="video/mp4" />
                </video>
                <h2 class="subtitle has-text-centered italic">
                    CamI2V - epipolar attention <br> only on reference frame <br> (similar to CamCo)
                </h2>
            </div>

            <div>
                <video autoplay controls muted loop>
                    <source src="https://github.com/user-attachments/assets/7e0cef9c-e6fc-42ea-8619-3b83cf459dc7" type="video/mp4" />
                </video>
                <h2 class="subtitle has-text-centered italic">
                    CameraCtrl
                </h2>
            </div>

            <div>
                <video autoplay controls muted loop>
                    <source src="https://github.com/user-attachments/assets/406a4134-995a-4493-8b0a-905d45af7098" type="video/mp4" />
                </video>
                <h2 class="subtitle has-text-centered italic">
                    MotionCtrl
                </h2>
            </div>

        </div>

    </section>

    <!-- attention comparison -->
    <section class="section hero">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">Attention Mechanisms for Tracking Displaced Noised Features</h2>
                <img src="https://github.com/user-attachments/assets/075ecfdf-d55a-42bb-9419-4031d5198b31" width="100%" />
                <h2 class="subtitle has-text-justified">
                    Temporal attention is limited to features at the same location of picture, rendering it ineffective for significant camera movements.
                    In contrast, 3D full attention facilitates cross-frame tracking due to its broad receptive field.
                    However, high noise levels can obscure deterministic information, hindering consistent tracking.
                    Our proposed epipolar attention aggregates features along the epipolar line, effectively modeling cross-frame relationships even under high noise conditions.
                </h2>
            </div>
        </div>
    </section>

    <!-- cam_to_ray -->
    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">Camera parameterization</h2>
                <img src="https://github.com/user-attachments/assets/e1e1186e-2a2a-4598-92a7-d56d75f4ad6d" width="100%" />
                <h2 class="subtitle has-text-justified">
                    Left: Camera representation and trajectory visualization in the world coordinate system.
                    <br>
                    Right: The transformation from camera representations to 3D ray representations as Pl&uuml;cker coordinates given pixel coordinates.
                </h2>
            </div>
        </div>
    </section>

    <section class="section hero">
        <!-- epipolar line and mask -->
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">Epipolar line and mask</h2>
                <img src="https://github.com/user-attachments/assets/ca7bd26d-f407-4616-a5a4-8ccc7873a484" width="100%" />
                <h2 class="subtitle has-text-justified">
                    Left: Epipolar constraint of the $j$-th frame from one pixel at $(u,v)$ on the $i$-th frame.
                    <br>
                    Middle: Epipolar mask discretized by the distance threshold $\delta$, so that only neighboring pixels in green are allowed to attend while those red lined are not.
                    <br>
                    Right: Multi-resolution epipolar mask adaptive to the feature size in U-Net layers.
                </h2>
            </div>
        </div>

        <!-- epipolar attention mask -->
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">Epipolar attention mask with register tokens</h2>
                <img src="https://github.com/user-attachments/assets/eb5f05ea-89ef-4fdb-8abc-00807b84d9fd" width="100%" />
                <h2 class="subtitle has-text-justified">
                    We specify query pixel by red point in the $i$-th frame for clarity.
                    Epipolar attention mask is constructed by concatenating epipolar masks along all frames.
                    We insert register tokens to key/value sequence to deal with zero epipolar scenarios.
                </h2>
            </div>
        </div>
    </section>

    <!-- pipeline -->
    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">Pipeline of CamI2V</h2>
                <img src="https://github.com/user-attachments/assets/cc344704-0985-409d-8236-501a16ebf75f" width="100%" />
            </div>
        </div>
    </section>

    <!-- <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>BibTex Code Here</code></pre>
        </div>
    </section> -->

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            This page was built using the 
                            <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> 
                            which was adopted from the 
                            <a href="https://nerfies.github.io" target="_blank">Nerfies</a>
                            project page.
                            You are free to borrow the source code of this website, we just ask that you link back to
                            this page in the footer. <br> 
                            This website is licensed under a 
                            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
                            Creative Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>

</html>